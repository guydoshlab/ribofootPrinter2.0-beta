from Bio import SeqIO
from Bio import Seq
import csv
import os
import pandas as pd
import sys
import tools

# Code obtained from mammalian_builddense leading to some extra code that will not be used for this script. 

# This is the region size and abundance script.
# It outputs a csv file with histograms of reads for each region of interest, along with total reads per region. It also reports options for normalization. These can be applied to the histograms or used in conjunction with total reads in each region to make pie charts showing where reads map.
# Inputs:
# fasta_in - This is the longnames fasta file.
# sam_in - This is the SAM input file of your mapped reads.
# outfile - output file with no csv extension
# subset_list - filter list file (e.g. subset_list.xlsx), if no filtering is required put "none". These are the only genes included if provided.
# smallsize - min footprint size, inclusive
# largesize - max footprint size, inclusive
# window - region around start and stop codons you want to require for being defined as start stop. 10 will give you 10 nt either side (21 total).

def main(fasta_in,sam_in,outfile,smallsize,largesize, window, subset_list):
	print("\nName of python script:",(__file__.split("/")[-1]))
	print("Total arguments passed:", len(locals()))
	print("Argument names: "+("; ".join(list(locals().keys()))))
	argkeys=list(locals().keys())
	argvals=[]
	for key in argkeys:
		argvals.append((locals()[key]))
	print("Argument values: "+("; ".join(argvals)))
	
	smallsize=int(smallsize)
	largesize=int(largesize)
	window=int(window)

	gene_list = None
	if subset_list != "none":
		selection = pd.read_excel(subset_list) # Provide list of genenames to subset dataset
		gene_list = set(selection["genename"].tolist())
		print ("Subset list was loaded containing " + str(len(gene_list)) + " genes.")

	genedata={}               # Creates an empty dictionary for storing outputdata.
	negativestrandreads=0       # Counter set to 0 to keep track of number of reads mapping to the negative strand. Reads mapping to the negative strand are not included in the outputdata dictionary.
	fsam=open(sam_in)           # Opening the SAM file generated by bowtie which contains information for each read aligned against the transcriptome.
	samgen=csv.reader(fsam,delimiter='	')  # Reading the tab-separated SAM file. The variable samgen is a list containing read alignment information.
	
	mappedreads={}              # Creates an empty dictionary for storing mappedreads. Keys for each region of interest.
	mappedreads["all"]=0	    # Total reads used in rpm.
	mappedreads["UTR5"]=0	   
	mappedreads["start"]=0	   
	mappedreads["CDS"]=0	   
	mappedreads["stop"]=0	   
	mappedreads["UTR3"]=0	   
	mappedreads["total"]=0		# This is all reads minus the those lost for not being in the subset. Same as subset. Total of other entries.
	
	footprintrange = list(range(smallsize,largesize+1))
	
	# Make lists to hold histograms.
	h_start=[0 for x in range(smallsize,largesize+1)] 
	h_stop=[0 for x in range(smallsize,largesize+1)] 
	h_UTR5=[0 for x in range(smallsize,largesize+1)] 
	h_CDS=[0 for x in range(smallsize,largesize+1)] 
	h_UTR3=[0 for x in range(smallsize,largesize+1)]
	h_total=[0 for x in range(smallsize,largesize+1)]

	# Make lists for normalization step below
	startcodon_list = []		# For 5'UTRs
	CDS_list = []				# For CDSs
	length_list = []			# For overall
	UTR3_list = []				# For 3'UTRs

	# Adding in transcripts information from the transcriptome FASTA file into outputdata dictionary 
	for record in SeqIO.parse(fasta_in, "fasta"):       # Reading in a fasta file using the SeqIO package.
		gene=record.id.split("|")[0]                    # Obtains genename from fasta file (ENSG000000001.12).
		genedata[gene]=["alias","sequence",{},"ORFstart","utr3start","transcriptlen"] #  Generates a list with 6 placeholders, only using last three for this script. 
		genedata[gene][3]=(record.id.split("|")[7]).split("-")[-1]	# ORF start
		genedata[gene][4]=(record.id.split("|")[8]).split("-")[-1]	# UTR3 start
		genedata[gene][5]=len(str(record.seq))               # Transcript length
		#genedata[gene][5]=record.id.split("|")[6]	                    # Transcript length

	print("Fasta processed.")

	# Loop through the samfile.
	counter = 0
	for read in samgen:
		counter +=1
		if counter % 1000000 == 0:
			print("Processed " + str(counter) + " reads...")
		if read[0][0] == '@':   	# Ignore header lines.
			continue
		if read[1] == '4':      	# A bowtie no match. 
			continue

		gene = read[2]              # gene name for mapped bowtie read, ie ENSG0000.
		startp = int(read[3]) - 1   # start position. Need to subtract 1 since genomic sequence starts at 1, but list structures for read summing start at 0.
		seq = Seq.Seq(read[9])      # sequence of the read
		length = len(seq)           # length of read
		startcodon=int(genedata[gene][3])
		stopcodon=int(genedata[gene][4])-3
		transcriptlength = int(genedata[gene][5])

		if (length<smallsize or length > largesize):
			continue

		# Remove negative strands
		if (read[1] == '16'):
			negativestrandreads+=1
			continue

        # Reads at start will not be counted if 5'UTR is less than or equal to window
		# Reads at stop will not be counted if 3'UTR is less than or equal to window-2.
		# Similarly, reads will not be counted in the UTRs unless the windows are 1 nt bigger than above.
		# Also, unlikely to be false, but CDS must be above a small threshold for start/stop and CDS to be counted.
		if (read[1] == '0'):	    # mapped reads that are going to be counted	
			mappedreads["all"]+=1
			if gene_list is not None and gene not in gene_list:  # Filter out any genes that are not in the subset list
				continue	
			h_total[length-smallsize]+=1
			mappedreads["total"]+=1

			if ((startp <= startcodon-window) and (startp+int(length)-1 >= startcodon+window)):     # test if start codon is completely overlapping a window; window is centered on the start codon with, for example, 4 nucleotides on either side of A.
				h_start[length-smallsize]+=1
				mappedreads["start"]+=1
			elif((startp <= stopcodon-window) and (startp+int(length)-1 >= stopcodon+window)):		# if the read does not fall within this region, the same window around the stop codon is checked
				h_stop[length-smallsize]+=1
				mappedreads["stop"]+=1
			elif((startp < startcodon-window)):														# if the read does not fall with start or stop codon region, the 5'UTR region is checked (this included everything from the start of the transcript up to 4 nucleotides before the A in AUG start codon)
				h_UTR5[length-smallsize]+=1
				mappedreads["UTR5"]+=1
			elif((startp > stopcodon-window)):														# if the read is not mapping to 5'UTR, 3'UTR is checked (region including 4 nucleotides after T of stop codon TAA TGA TAG up to the end of the transcript.
				h_UTR3[length-smallsize]+=1
				mappedreads["UTR3"]+=1
			else:																				# everything that has not been accounted for up to here will be counted as coding sequence
				h_CDS[length-smallsize]+=1
				mappedreads["CDS"]+=1
					
			# For each read, store the corresponding gene's length info.
			# Store information for normalization step below
			startcodon_list.append(startcodon)
			length_list.append(transcriptlength)
			CDS_list.append(stopcodon - startcodon+3)
			UTR3_list.append(transcriptlength - stopcodon-3)
			
	if mappedreads["all"] == 0:
		print("Error: no genes expressed")
		exit()
		
	# Normalization factors that can be applied to the histogram, or used to report abundances if applied to the sum of all readlengths. 
	normtable={}
	normtable["normalization_option"]=["UTR5","start","CDS","stop","UTR3","total"]
	normtable["raw"]=[1,1,1,1,1,1]						# Since the reported values are raw already, just 1s here.
	normtable["probability_density"]=[0,0,0,0,0,0]		# This is a standard probability density; the histogram integrates to 1.
	normtable["rpm_density"]=[0,0,0,0,0,0]				# This is the same shape as not normalized (raw reads), but in units of rpm to facilitate comparison across samples of differing depth. 
	normtable["rpm_length_density"]=[0,0,0,0,0,0]		# This is another normalization that takes the length of the average length of features in a particular account so it's normalized by length density, determined by read data. This normalization emphasizes short regions (i.e. start/stop).

	# Calculate a weighted average length of each feature based on mapped reads. Start/stop are definitionally 2*window+1.
	if len(startcodon_list) == 0 or len(length_list) == 0 or len(CDS_list) == 0 or len(UTR3_list) == 0:
		print("Reads are going to be divided by 0 error. Normalization not possible.")
		exit()
	UTR5_norm = sum(startcodon_list) / len(startcodon_list)
	start_norm = window + window + 1
	stop_norm = window + window + 1
	CDS_norm = sum(CDS_list) / len(CDS_list)
	UTR3_norm = sum(UTR3_list) / len(UTR3_list)
	total_norm = sum(length_list) / len(length_list)

	print(str(mappedreads["all"])+" all reads mapped to sense strand transcriptome (before any subsetting).")
	print(str(mappedreads["UTR5"])+" reads mapped within the 5'-UTR region which has a weighted average length of " + str(round(UTR5_norm)) + " nt.")
	print(str(mappedreads["start"])+" reads mapped within the start codon region which has an average length of " + str(round(start_norm)) + " nt.")
	print(str(mappedreads["CDS"])+" reads mapped within the CDS region which has a weighted average length of " + str(round(CDS_norm)) + ".")
	print(str(mappedreads["stop"])+" reads mapped within the stop codon region which has an average length of " + str(round(stop_norm)) + " nt.")
	print(str(mappedreads["UTR3"])+" reads mapped within the 3'-UTR region which has a weighted average length of " + str(round(UTR3_norm)) + " nt.")
	print(str(mappedreads["total"])+" reads mapped to transcriptome (includes any subsetting) which has a weighted average length of " + str(round(total_norm)) + " nt.")

# Normalize
	if mappedreads["UTR5"] == 0:	
		print("Warning: No reads in 5'-UTR region. Normalization cannot be performed.")
	else:
		normtable["probability_density"][0]=1/mappedreads["UTR5"]
		normtable["rpm_density"][0]=float(1E6)/(mappedreads["all"])
		normtable["rpm_length_density"][0]=1/(mappedreads["all"])*float(1E6)/UTR5_norm

	if mappedreads["start"] == 0:	
		print("Warning: No reads in start region. Normalization cannot be performed.")
	else:
		normtable["probability_density"][1]=1/mappedreads["start"]
		normtable["rpm_density"][1]=float(1E6)/(mappedreads["all"])
		normtable["rpm_length_density"][1]=1/(mappedreads["all"])*float(1E6)/start_norm

	if mappedreads["CDS"] == 0:	
		print("Warning: No reads in CDS region. Normalization cannot be performed.")
	else:
		normtable["probability_density"][2]=1/mappedreads["CDS"]
		normtable["rpm_density"][2]=float(1E6)/(mappedreads["all"])
		normtable["rpm_length_density"][2]=1/(mappedreads["all"])*float(1E6)/CDS_norm
		
	if mappedreads["stop"] == 0:	
		print("Warning: No reads in stop region. Normalization cannot be performed.")
	else:
		normtable["probability_density"][3]=1/mappedreads["stop"]
		normtable["rpm_density"][3]=float(1E6)/(mappedreads["all"])
		normtable["rpm_length_density"][3]=1/(mappedreads["all"])*float(1E6)/stop_norm
		
	if mappedreads["UTR3"] == 0:	
		print("Warning: No reads in 3'-UTR region. Normalization cannot be performed.")
	else:
		normtable["probability_density"][4]=1/mappedreads["UTR3"]
		normtable["rpm_density"][4]=float(1E6)/(mappedreads["all"])
		normtable["rpm_length_density"][4]=1/(mappedreads["all"])*float(1E6)/UTR3_norm

	if mappedreads["total"] == 0:	
		print("Warning: No reads in total region. Normalization cannot be performed.")
	else:
		normtable["probability_density"][5]=1/mappedreads["total"]
		normtable["rpm_density"][5]=float(1E6)/(mappedreads["all"])
		normtable["rpm_length_density"][5]=1/(mappedreads["all"])*float(1E6)/total_norm

	# Create a total reads at bottom of columns that can be used for making abundance pie charts.
	footprintrange.append("all_lengths")
	h_UTR5.append(sum(h_UTR5))
	h_start.append(sum(h_start))
	h_CDS.append(sum(h_CDS))
	h_stop.append(sum(h_stop))
	h_UTR3.append(sum(h_UTR3))
	h_total.append(sum(h_total))
	
	writerfile=open(outfile+".csv", "w")	# writing out the data row by row
	writer = csv.writer(writerfile)
	writer.writerow(["Use readlengths for histogram or all_lengths for pie chart"]+["" for x in range(11)])
	writer.writerow(["range"]+footprintrange)
	writer.writerow(["UTR5"]+h_UTR5)
	writer.writerow(["start"]+h_start)
	writer.writerow(["CDS"]+h_CDS)
	writer.writerow(["stop"]+h_stop)
	writer.writerow(["UTR3"]+h_UTR3)
	writer.writerow(["total"]+h_total)
	writer.writerow(["" for x in range(12)])
	writer.writerow(["Normalization factors - multiple by read lengths for histogram or all_lengths for abundance pie chart"]+["" for x in range(11)])
	for key in normtable.keys():
		writer.writerow([str(key)]+normtable[key]+["" for x in range(5)])
	writerfile.close()
	tools.transposecsv(outfile)	

	fsam.close()

# For running from the command line, the code below pulls in the variables and calls main.
if __name__ == "__main__":	
	if len(sys.argv)!=8:
		print("Wrong number of inputs.")
		exit()
	main(sys.argv[1],sys.argv[2],sys.argv[3],sys.argv[4],sys.argv[5],sys.argv[6],sys.argv[7])